{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d3JIcxBos1Y",
    "outputId": "222b1600-de08-46ff-f8d9-911b752383d9"
   },
   "outputs": [],
   "source": [
    "cwd = %pwd\n",
    "print(f'Current Working Directory: {cwd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sa-7PqHLo31O",
    "outputId": "1c8f8bf9-6dd4-488a-96e1-a3f4d6f31406"
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VQ2hNsbos6v"
   },
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "%%capture\n",
    "import os, re\n",
    "\n",
    "import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "!pip install sentencepiece \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "!pip install --no-deps unsloth\n",
    "\n",
    "!pip install transformers==4.55.4\n",
    "!pip install --no-deps trl==0.22.2\n",
    "\n",
    "%%capture\n",
    "!pip install \"timm==1.0.19\"   # Only for Gemma 3N\n",
    "!pip install \"gdown==5.2.0\"\n",
    "!pip install tqdm\n",
    "\n",
    "import torch; torch._dynamo.config.recompile_limit = 64;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3lS5wmAqfql",
    "outputId": "c001c4ba-586d-4149-92e3-bfcdcc9a307c"
   },
   "outputs": [],
   "source": [
    "# Log in to Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "def get_hf_token():\n",
    "    # Check if running in Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import userdata\n",
    "        hf_token = userdata.get(\"HF_TOKEN\")\n",
    "        print(\"Running in Colab: using userdata for HF_TOKEN.\")\n",
    "    except ImportError:\n",
    "        # Not in Colab, try to load from .env file\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        hf_token = os.getenv(\"HF_TOKEN\")\n",
    "        print(\"Not in Colab: using .env for HF_TOKEN.\")\n",
    "    if not hf_token:\n",
    "        raise RuntimeError(\"HF_TOKEN not found in Colab userdata or .env file.\")\n",
    "    return hf_token\n",
    "\n",
    "\n",
    "hf_token = get_hf_token()\n",
    "login(token=hf_token)\n",
    "print(f\"HF Token: {hf_token[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4gRk_04omb_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "def run_cpu_inference(model_name, image_url, prompt):\n",
    "    \"\"\"\n",
    "    Loads a fine-tuned model from Hugging Face using only the standard\n",
    "    transformers library and runs inference on a CPU.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model for CPU: {model_name}\")\n",
    "\n",
    "    # Use the standard AutoProcessor and AutoModelForCausalLM classes\n",
    "    # This is the Hugging Face standard for loading models.\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16, # Use float16 for a smaller memory footprint\n",
    "        low_cpu_mem_usage=True,    # A transformers flag to be more memory efficient on CPU\n",
    "    )\n",
    "    print(\"✅ Model and processor loaded successfully.\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Opening image: {image_url}\")\n",
    "        if image_url.startswith(\"http://\") or image_url.startswith(\"https://\"):\n",
    "            image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
    "        elif os.path.exists(image_url):\n",
    "            image = Image.open(image_url).convert(\"RGB\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image not found at {image_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load image: {e}\")\n",
    "        return\n",
    "\n",
    "    # The chat template is part of the processor's configuration,\n",
    "    # so we can apply it directly without any special functions.\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # The processor handles both text and images\n",
    "    inputs = processor(\n",
    "        text=processor.apply_chat_template(messages, add_generation_prompt=True),\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # Note: We DO NOT move the inputs to \"cuda\". They will stay on the CPU.\n",
    "\n",
    "    print(\"⏳ Running inference on CPU (this may take a moment)...\")\n",
    "\n",
    "    # Generate the output\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the result\n",
    "    result = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Post-process the output to get only the assistant's response\n",
    "    assistant_start = result.rfind(\"model\\n\") # Use rfind to get the last occurrence\n",
    "    if assistant_start != -1:\n",
    "        generated_text = result[assistant_start + len(\"model\\n\"):]\n",
    "        # Take the first line to avoid repetitions\n",
    "        generated_text = generated_text.split('\\n')[0].strip()\n",
    "    else:\n",
    "        generated_text = \"Could not extract generated text.\"\n",
    "\n",
    "    output_data = {\n",
    "        \"image_source\": image_url,\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_description\": generated_text\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- Inference Result (JSON) ---\")\n",
    "    print(json.dumps(output_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490,
     "referenced_widgets": [
      "da9f1788201546e08ab26045ddb3d969",
      "cfc27b161e964d94bc5efe4f66a2d268",
      "ea56b086f11a4e62acfc194bb3725497",
      "83591d50230c45a89addac051eb7259f",
      "9f810a5068a64f04b6471d81ba9a7354",
      "28fe78c3fc4f48e68443b77ba432dfd4",
      "cb69f74b4aad41bda916c89d7f945514",
      "28a0f5f2df864e2bb4b954995a311e5b",
      "d846c3c776e2410080e523818a00285f",
      "46334965bce044b0abd3507daa356b9f",
      "7bc2ac8d9316487680178d5b3940685b",
      "567494ac8536419d8d137edd99bab6fa",
      "22436e79ab4c48a48c37306353c95777",
      "0ab4d9b35a8e405c88e9db372e8219d8",
      "d3f764ec325f488abf1ba25b6a8669cb",
      "1a2b453c58b14064864c6ca7635b3557",
      "435dec7a5932457dafade97d618afebe",
      "d60cee419c164153ae5fa3eb605f5953",
      "3e38d264ed3a44a79118f53b5096c526",
      "57477fc7aaae49e89b3c238b7a35a3a2",
      "33115c5624b648f983f1952c1c7edb70",
      "f009df1ddebd48b08a96e5ea5812769c",
      "a6e30c726d014bbf853034200fdbc2c6",
      "c6631786b64242e5ad49898dd10bda30",
      "a93998f89b7e4d84bd382a136ca66937",
      "26ed48eb0d354c4d944ebe5e04bf7948",
      "1855eec10593470581789401f7e35556",
      "067a9bec635643afa1ea822ab0fe8bae",
      "4a7d1d8f1fb3435d9ebe6c8bd602eb74",
      "a4084ab809c24e8fbfbfc64efb2f9caa",
      "8ff59d5ccc3444ff9c7ff79d983b2537",
      "c888f63b75bc4f9ca607e84e2e51006e",
      "af2377209d4f408990240b2bf8eb0f62",
      "c377fefd606f4b39b93deba0a438c082",
      "f5b6e9a7be9e4f40b9acb968c1a96b45",
      "b8c10722ef07418a85e9c5eb48025645",
      "8d01ca739a974176ba90ee59de508d2f",
      "99f540dc9e77409da3fd9d86424695f5",
      "bfb6a6f67aa844d880a62831c993fc27",
      "aea2598dca434131b663a76c247f90a3",
      "518d5e7fe1a04f7a9eb78ba2b5d7f834",
      "41eb07a2ef654f2f9245027c86b2f7fe",
      "e6688bb9ddb14d1589bda722d6b610f9",
      "16ccb2c3808f4257928f56db76580b86",
      "8a958b9d203344979254e4294c3c2284",
      "71f2b327fa174f8484b868b59407ac44",
      "c37d50b851bc494895c9e7839df21012",
      "5010cacecdb24249a774ef2613be38fa",
      "ff23065c0c8a44ab88584c91605ef2e1",
      "741e448792ff4a7db4c2d8fed991b778",
      "d8f100843278440eb870161d28713ad5",
      "2a202125d06346db99d500c961c4969e",
      "31914888600f4498a9cf9fa834a7c9f6",
      "6537a3ed00964e0f9f6cd4e272b99ca5",
      "4d4f4da15bc64bbea364087ce49cafa4",
      "6abe71961de34512bb2f165f65cef8cd",
      "9672ad26cf574282a511e3bf008ec262",
      "3b0f3720ca464b3f86a430ef34baa22d",
      "501a6532e8c1434a840b3e6a2199ac50",
      "9a05fef370c54129a82976071bd2bf68",
      "309fbcc8322947d5bd39e0e813901de3",
      "c9d03a997bdb4193b643eb9e29e98163",
      "dc99861a165843b588cffdfd90f72082",
      "86606f1479894cd5a494517ec747f411",
      "8463be2c1ab94cd298ebc8c06fc36edb",
      "a7503b018cfb4c51bd6945e8a9e2c7a3",
      "9e19aec3b8054e8ab2c9c97e364f1dce",
      "d49363d9d85f4618a854db1fb555d75b",
      "5c17ceb6503d438085578a8027e1da61",
      "aa20d47404fb425dad63ae2c7e925e19",
      "d486e48c47374cba84da7ccfdc182cbd",
      "aa3b8374bb0b4eefb14f1fff0244d69b",
      "198e4b3bb25e4118913d6593832158ce",
      "3f8965df8f42435da5c5344b1dc24279",
      "c5117270303d476db42583ab3d5b02f4",
      "9975f1ddd7c84ade94283894c22d3754",
      "747a42c4e9864de8b4f3de28f04444a5",
      "9099ee00a48a4eb2a455fb2c3b730716",
      "2fcc9f1893294468b900a0dd9aac0e42",
      "1d6eca57eb3246bbb82d2d6c10becf22",
      "75ba3411f8924d289e4e1e3cf83978a7",
      "93cd4de2b17e4c58b41dd506b5a2fb10",
      "304e28327b63460f8a526a3ff73d75de",
      "f3167a80653949fb9e27d51938eacd72",
      "227ae648332a4251b13eae46a58799e2",
      "4581dbc5edd3461b8766580942e95783",
      "a13ab3f80c6b4f03b6faac2482ea3684",
      "da889e96381c4683844ab06ebf35ad60",
      "8c5316891fa648b5a64b170417f51e0e",
      "b8d252d05e4e4e0897a418e932cc66e7",
      "0ec33859bfb04c419849f9b50b51b546",
      "2b043eda01174a16b56281a416853a5c",
      "af8fabbd033e4c32b9187071ff60159c",
      "d2eb3938769141908598101657312451",
      "e6411bbbf249464eb27eb112c52339c1",
      "ba6b300672fa418d925e53d385641988",
      "b1c1f4d3d86a4624b9574b30f70bbd5c",
      "758d14157ec94493a62048cf7ef1a36b",
      "1cb21012f67247e28d4ed8ad739a305d",
      "1ae5bea467b74ebfbc487f2391d4073f",
      "3c3f22e0d39b4f18a0e5d18e1766f2cf",
      "f1d02be3229448a3b6ccfee039918ef9",
      "ed34db31434b49ea878e7eb3eb0109b8",
      "86c49c9149154d708bf1403b8d4efaea",
      "dcadee7ed8e843d99c8de257fec75bf9",
      "d83fe36afbed4890bb62db70bcac7867",
      "e12fdaf3b731447b9bf5cde09e049b2f",
      "8c760129b23f4877918836577ada51df",
      "01c3b64e3eb5479aa17412370fd01fbb",
      "02231c4c9cef4f95bf10c557063f15b7",
      "f29ad910418247808fa214fd921a09da",
      "055469bc2b7149a8916f5c9b6c5c4a64",
      "2366aaa06e964750a060825bca6c3901",
      "902eafec8a1e4fc6ab17ac6d2270bca6",
      "e9550293cede4f8482ef606c1a402806",
      "cc9d5bee6e76489381ff36c91fde59f5",
      "a564420478254b518f510ac06a17c129",
      "03eaa9c32578479d9c73585917822424",
      "d7c778e49a1042c3971b719856c3e7e1",
      "6fb28f28b6eb48e4b76f7471130dea5b",
      "bbce66e102154e62ac76c4cc8b7ad3d9",
      "38a4b976babc4f69bb976e917d3a4aec",
      "00d1a6b854234dad805644b1e3a77343",
      "d43b2532a8b0477b99ce8fb72359e418",
      "5e2092f4e0c3475f86bc4288f66fc68a",
      "ecd7566ea52347c8a2b4edf93adebd8f",
      "cf1b812598344032ab01723dbb534300",
      "ec8cb5971fce4cd782b6bb6b06101d68",
      "84ef12f9245242fcad607ae21200159e",
      "37b0e052d1774fff9244d4538d6c0dcf",
      "8853f734423d45da95d110929d11a4cc",
      "2fa784a400d2417fb26e17a23f0c537e",
      "04a68e0a15e4499f8c513993f55d9648",
      "59c4d8318e24423395bd4f021a910591",
      "a2ead327f5d641878ad5f13155ea0475",
      "d24761c5dfc2443685d04ff88bd0a078",
      "39a41efaf0614d9d9353691d5e1cebd0",
      "9c651c2555804fffbad2450bfc61a19f",
      "5f772377905f413b94e91520ae7a936d",
      "d80d2571ba8c4de9a089fd3c7443cee0",
      "5880d7b28dde47959abafc9d0dd91200",
      "930b8045426a4af8954551cbf7b56621",
      "919e16ecd73a496a98362e885f5543ca"
     ]
    },
    "id": "9_Iv_Daoomee",
    "outputId": "f5eb0608-183f-4119-a2b7-58b434479227"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mazqoty/gemma-3n-vizWiz-finetuned\"\n",
    "IMAGE_URL = \"http://images.cocodataset.org/test-stuff2017/000000000416.jpg\"\n",
    "PROMPT = \"Write a short, clear description of this image.\" # Use the simple, effective prompt\n",
    "\n",
    "run_cpu_inference(MODEL_NAME, IMAGE_URL, PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hMV-hpromhK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6by9GgDeomkP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
