{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS_gspEgUGCb"
   },
   "source": [
    "### Mount G Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-EZlCYQ-cu-",
    "outputId": "eb907c95-4295-41d8-8cf1-3f054dd7e1f0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPtqct1UUYlx"
   },
   "source": [
    "### Setup Project\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "h0Zzhv6L-fha",
    "outputId": "73d86058-8d2c-4f28-b913-a483417d8b38"
   },
   "outputs": [],
   "source": [
    "# Upload and unzip project code\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Upload the zip file\n",
    "if not os.path.exists('/content/fineTune.zip'):\n",
    "  uploaded = files.upload()\n",
    "  zip_name = list(uploaded.keys())[0]\n",
    "else:\n",
    "  zip_name = '/content/fineTune.zip'\n",
    "  print(\"FineTune.zip already exists\")\n",
    "\n",
    "# Unzip into the main content directory\n",
    "!unzip -q {zip_name}\n",
    "\n",
    "# IMPORTANT: Add your project's root directory to the Python path\n",
    "# Add your project's root directory to Python's search path\n",
    "project_root = '/content/fineTune'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"‚úÖ Added '{project_root}' to Python's path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1eaObc2_ZG0",
    "outputId": "d44a750a-93c0-4ba9-c8ac-bb10784e2956"
   },
   "outputs": [],
   "source": [
    "cwd = %pwd\n",
    "print(f'Current Working Directory: {cwd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrvC_LDBUaLg"
   },
   "outputs": [],
   "source": [
    "# %rm -r /content/fineTune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGrD0BJwrZz_"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZ46Q79B-pUY"
   },
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "\n",
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.55.4\n",
    "!pip install --no-deps trl==0.22.2\n",
    "\n",
    "\n",
    "!pip install -q -r /content/fineTune/colab_requirements.txt\n",
    "\n",
    "%%capture\n",
    "!pip install \"timm==1.0.19\"   # Only for Gemma 3N\n",
    "!pip install \"gdown==5.2.0\"\n",
    "\n",
    "import unsloth\n",
    "# print(unsloth.__version__)\n",
    "import torch; torch._dynamo.config.recompile_limit = 64;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnTuDE3tULFS"
   },
   "source": [
    "### GPU | Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNb0woXyAxSp",
    "outputId": "66873237-a24a-4e33-c6bd-da734cb68c77"
   },
   "outputs": [],
   "source": [
    "from utils import get_gpu_status, get_gpu_usage_stats\n",
    "get_gpu_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNOQ5hkuj6Yd",
    "outputId": "7c8cf21c-af66-4c84-af09-34d3e04d9320"
   },
   "outputs": [],
   "source": [
    "from utils import capture_key_dependency_versions\n",
    "capture_key_dependency_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvviJI-sUS_v"
   },
   "source": [
    "### Setup *Hugging Face*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vU7qoBXBBBUw",
    "outputId": "61b5304f-b09c-40c6-948d-3a1e2f0b0ebc"
   },
   "outputs": [],
   "source": [
    "# Log in to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(token=hf_token)\n",
    "\n",
    "print(f\"HF Token: {hf_token[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VoOAtGZUgBq"
   },
   "source": [
    "### Modify *'configs.yaml'* for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLds36lRBEcI",
    "outputId": "248ddc74-cc46-49a7-c270-92d29e0d44e3"
   },
   "outputs": [],
   "source": [
    "# Modify config paths for a hybrid Colab + Drive environment\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "CONFIG_FILE_PATH = '/content/fineTune/configs/configs.yaml'\n",
    "\n",
    "# --- DEFINE STORAGE LOCATIONS ---\n",
    "# 1. Temporary, fast storage on the Colab machine itself\n",
    "COLAB_DISK_BASE_PATH = '/content/fineTune'\n",
    "\n",
    "# 2. Permanent, persistent storage in your Google Drive\n",
    "DRIVE_BASE_PATH = '/content/drive/MyDrive/FineTunning/Gemma'                  # A dedicated folder in your Drive\n",
    "\n",
    "# Create the permanent storage directory in Google Drive if it doesn't exist\n",
    "from fineTune.utils import make_clean_dir\n",
    "\n",
    "make_clean_dir(DRIVE_BASE_PATH)\n",
    "make_clean_dir(f'{DRIVE_BASE_PATH}/models')\n",
    "make_clean_dir(f'{DRIVE_BASE_PATH}/logs')\n",
    "\n",
    "\n",
    "with open(CONFIG_FILE_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# --- UPDATE PATHS IN THE CONFIG ---\n",
    "\n",
    "# 1. DATASET PATHS: Point to the large, fast, TEMPORARY Colab disk\n",
    "config['dataset']['paths']['base_dir'] = f'{COLAB_DISK_BASE_PATH}/dataset/vizWiz'\n",
    "config['dataset']['paths']['temp_zip_dir'] = f'{COLAB_DISK_BASE_PATH}/dataset/vizWiz/zips'\n",
    "config['dataset']['paths']['images_dir'] = f'{COLAB_DISK_BASE_PATH}/dataset/vizWiz/images'\n",
    "config['dataset']['paths']['annotations_dir'] = f'{COLAB_DISK_BASE_PATH}/dataset/vizWiz/annotations'\n",
    "config['dataset']['paths']['processed_dir'] = f'{COLAB_DISK_BASE_PATH}/dataset/vizWiz/processed_for_tuning'\n",
    "\n",
    "# 2. FINE-TUNING OUTPUTS: Point to your PERMANENT Google Drive\n",
    "config['fineTune']['adapters_output_dir'] = f'{DRIVE_BASE_PATH}/models/gemma-3n-adapters'\n",
    "config['fineTune']['trainer_output_dir'] = f'{DRIVE_BASE_PATH}/models/training_checkpoints'\n",
    "config['fineTune']['merged_model_output_dir'] = f'{DRIVE_BASE_PATH}/models/gemma-3n-merged-model'\n",
    "config['fineTune']['logging_dir'] = f'{DRIVE_BASE_PATH}/logs'\n",
    "config['fineTune']['dataset_path'] = f'{COLAB_DISK_BASE_PATH}/dataset/vizWiz/processed_for_tuning'\n",
    "\n",
    "# Write the updated configuration back to the file\n",
    "with open(CONFIG_FILE_PATH, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(\"‚úÖ Configuration file updated for a hybrid Colab + Google Drive environment.\")\n",
    "print(\"*\"* 20)\n",
    "print(\"--- New Config ---\")\n",
    "print(\"*\"* 20)\n",
    "!cat {CONFIG_FILE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ARz-tDGLLsJL",
    "outputId": "a233205e-a182-4f8c-e4fb-5ae464f96e0e"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yGQeA9CNV_A",
    "outputId": "9fa3d34c-eada-43f1-db54-5d2a07779025"
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pug5qZMP_IqH"
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f548983fb1b04d57a7148908a69cef87",
      "ecf58d57392b4e03bdad6f3a1f047d6d",
      "b702ae98dd704912aaf343a596b6307a",
      "bd5b185814874c598fa4740697cb99b6",
      "f5a938b445e84c43b681653f95437500",
      "1f386a60eadd4a3380f93766452b5147",
      "b006135b67f14281ae64e9a3b47585fa",
      "197ab68bd5e94e0c9a495fa90141603e",
      "e2e8e9b27beb4f4ca232e0b40a2eb3a3",
      "1ce50fb50dfd446f965097fb3db62884",
      "0b0ae717c6464694bac144e54befa27a",
      "9608dbea46ca40a8bef518953b6f9379",
      "d09e1cccefeb42e4ae5b09ef1d032682",
      "d53b755ea00e4123bbcd0e7126f9ca9d",
      "98892201169c462d834f4ea038b815ac",
      "d623e05b19e04ed093271685f058ce4f",
      "b61d5fda81ba4dc5afb22afb5371111d",
      "0d97553836d64cf18b0338e9e63e3605",
      "aae07158320f421ea19b3f64d6d72105",
      "982ca4bab80b41fa8730c02eca9dcb6c",
      "94bbc7ee4fc44ec5b04c67fdd2af8a64",
      "c7597aa2c2e44d75a424f40e9c4505e9",
      "95ce19b7d5e84086999b6a7ed42da7a2",
      "6bcda3dd72ea46dea764147cd894b4c8",
      "06bdf651e692417cb73e0d40f5dc8973",
      "fbbaec9f53ca4cdcb2a4949bf8fa159b",
      "f7dcacf1e34e4fca8671930265b3417e",
      "2e3189ae19574e9e986412e1c312f2c1",
      "b15f04ef3d064cd2a257535b0cf8c08d",
      "c50b565504a14c27aa83781574490832",
      "d35b8b7f618d4eb78f75175e5978e440",
      "acfc7a6c82ea4b3bbc04fe74a0537318",
      "cc67cd4cbfbd40fea586d92268315a2c",
      "80cbd2d3dfc8401ea74eff216012e6a6",
      "dd308fc99b5147ebb4be0ba6addc749f",
      "2d1deaa2bbd34d9c85f3ece8d27271bb",
      "d66ed9e4baa44c7e8ab8caa4d8e9540a",
      "1853b6d13774489a842bf94b7342b7c4",
      "757e68619b394adf812ad9ff7d0b7139",
      "307bc9f14d384b97a9129c077c901c9e",
      "20c671b045aa485380bea80fb1d1508f",
      "e6b3d255954e455d84f2d1dd2fe4f228",
      "00e63ffd7cb949a3ad5808ed250db8ef",
      "788be7b869864a76927d43bfc76b5962",
      "4cce7e9a9d6548b1b9bd4fecf06a0c23",
      "2dde4b0c375243bfaa99217639d53d98",
      "2a477baf72774b8897bc0bf24e8a19b0",
      "23142d906c1e4e9ebe74984bb45cde53",
      "ab3b85b295cb4e1781a55dfcd3f5ec7f",
      "12db70f1a8994ac488ffbc7b5631ad87",
      "3636294f6b0f49a790044b6d0bb12f22",
      "0e8c9e3b8c324dd0811767ee95114ec3",
      "24d0ca17bf774cd09d910ea54f985ed9",
      "bcaa9a86c9e54cec825ba171f1e9d489",
      "a1bbae4e4daf4ae9bce2c0ed76b198b9",
      "45a515cd1da843b68d62fe4f16047259",
      "7533b2f2fac540aca086f07dbd140041",
      "716f7734269243a688e89431cb5f7e35",
      "2976ff2ea4a14a768e55609978e5675f",
      "969c6cc2b60c42068e321618cbab2834",
      "6418b3c84f494e22b0b2aaa95cf089de",
      "f4d24690eaae4cfeb6a99ea60b8af9ad",
      "fdb3d352510f453cbd882ef09986736c",
      "8f665ae559574de5805d74972d40fd35",
      "83536ab1ba874029b364e32db0942e9e",
      "7f0cd8d574c54ca78953ed045cb30448",
      "a41fc3ee56f94c63a0e5f0ce521771e9",
      "f7fabf1cb10349bbadfa1a3b2f1512f7",
      "e32f873182454ff99c3b6bfe09439bb1",
      "6df0562bf63b4348925dba8de7874e41",
      "9c0afbd890964d5ebee0c59064b5cc62",
      "a118b58ef59f4206a8328f0a8a609def",
      "f79026a0d4ac46d38a655b20930b6715",
      "77a7de1bd2c946b4b4090c8f9a2a6123",
      "1b675ff8e659492f9bd72ac28d3ab353",
      "7ef08c4d95434014a324757ea02f3100",
      "a127c7d6bce34eb4ac3f50fc6fea67ea",
      "9a6afce7889a484385eb5cf3124a77aa",
      "9dd0cffdc0e94053a776192d0b0faec3",
      "6bdfc885484241799c7f8153152685c3",
      "3659412016084f348b49e1f72ddb16da",
      "dc0de439635d443e8653ab7619b66f11",
      "25e2f56f16b340e580c6bd7f4fa60494",
      "2c38062855cd491fb4697062e9fd39c3",
      "8837a8d0b0c1431886c9281c1eec172c",
      "390ce4dd7b734a87832219ac8be2d5e0",
      "2511423926234bc7abbf3dafd2a207e9",
      "5181bea377b343cc9f788429d9390e6b"
     ]
    },
    "id": "lK0wYi6wJmRa",
    "outputId": "2d550085-1d8c-447c-f9b2-44c5cca937e3"
   },
   "outputs": [],
   "source": [
    "# Run the data preparation and transformation\n",
    "import prepare_data\n",
    "\n",
    "# Run the main function from `prepare_data.py`\n",
    "prepare_data.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnzI45s0_As7"
   },
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UkZmmUFOLmaN",
    "outputId": "0e8f6e99-a76c-4d2a-90af-54b916d58e53"
   },
   "outputs": [],
   "source": [
    "import visualize_data\n",
    "\n",
    "visualize_data.VIEW_SPLIT = \"train\"   # The split to view ('train' or 'validation').\n",
    "visualize_data.VIEW_INDEX = 12        # The index of the example to view.\n",
    "\n",
    "# Run the main function from `visualize_data.py`\n",
    "visualize_data.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCes3Eu3-6-o"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8QNWn84liA1"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6P7Kmw4nlUzs"
   },
   "outputs": [],
   "source": [
    "# --- Unsloth must be imported before transformers, trl, peft ---\n",
    "from unsloth import FastVisionModel  # FastLanguageModel\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from unsloth import get_chat_template\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import yaml\n",
    "from data_pipeline import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# --- Local Utilities ---\n",
    "from utils import (\n",
    "    DetailedLoggingCallback,\n",
    "    display_evaluation_summary,\n",
    "    display_training_summary,\n",
    "    get_gpu_usage_stats,\n",
    "    make_clean_dir,\n",
    "    push_merged_model_to_hub,\n",
    "    save_merged_model_locally,\n",
    "    setupTensorboard,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDHLPNDcmC42"
   },
   "source": [
    "#### Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqNLnX-zlU_N"
   },
   "outputs": [],
   "source": [
    "# --- Load Configuration ---\n",
    "CONFIG_FILE_PATH = \"fineTune/configs/configs.yaml\"\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_FILE_PATH, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Failed to load configuration file '{CONFIG_FILE_PATH}': {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLxs2kJEmJgr"
   },
   "source": [
    "#### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-ina31_lVDC"
   },
   "outputs": [],
   "source": [
    "# --- Define Constants ---\n",
    "MODEL_ID = config[\"fineTune\"][\"model_to_use\"]\n",
    "PROCESSED_DATASET_PATH = config[\"dataset\"][\"paths\"][\"processed_dir\"]\n",
    "FINAL_ADAPTERS_OUTPUT_DIR = config[\"fineTune\"][\"adapters_output_dir\"]\n",
    "TRAINER_OUTPUT_DIR = config[\"fineTune\"][\"trainer_output_dir\"]\n",
    "LOGGING_DIR = config[\"fineTune\"][\"logging_dir\"]\n",
    "\n",
    "# Assume these are defined earlier in your script\n",
    "start_gpu_memory: float = 0.0\n",
    "max_memory: float = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ev66MhGmNZp"
   },
   "source": [
    "#### Prepare Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI1TbL4elVG3"
   },
   "outputs": [],
   "source": [
    "# --- Prepare Directories ---\n",
    "make_clean_dir(FINAL_ADAPTERS_OUTPUT_DIR)\n",
    "make_clean_dir(TRAINER_OUTPUT_DIR)\n",
    "make_clean_dir(LOGGING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-3sRakBmT8d"
   },
   "source": [
    "#### Load Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453,
     "referenced_widgets": [
      "c4406b3fa65740499bf6c9058218ac11",
      "5fca22c6441d4be0b9c7b46f619e8e51",
      "e88e732186a7420ca519b1b402665e70",
      "5e0ed0992ee54e09b8d463671c97d01d",
      "a96838fca54b4a01b6a26ad386814e35",
      "eba9d000694b4e17aec01cd6f696664c",
      "1a24578917c24cba9506f4e9c0ab9405",
      "1fb890297ada40db93dd6bfe3e6c73e2",
      "f82491a16648415694c3b2356681458a",
      "22b4ee06de304c91a89801b1158c301d",
      "29905981b30443ad96ae0769ec1de11c",
      "526133ea06ef418fbbfe9ebd2cc51a67",
      "49c84d8bdba84794a1f31e4417d1a529",
      "69ec7bac589f4616a75e5a56939a6dfe",
      "e3cc4f9d466a4e48abf567cf6ee823e2",
      "7f99d88e61d244f49e71cf8b6dd0d16a",
      "b5c246e4b4804742863e80ebb23d6973",
      "d3fb65b02bbd44688dc7e950419044f5",
      "3bf60ccc981c405892e133f452fbade0",
      "81b581c345284bdebe34f801df806df0",
      "91199273001a47b6b5027ab43602b117",
      "8fd4769a4ec04b0483b682f6affb56bf",
      "d4e9177ebd4c43eda8f13ce1847a1f04",
      "410b0fc5490d4ede9da947af8311a5b8",
      "e7e3b7f4aea54635a4f03665f83f8b74",
      "50cb959f873540e59354905c932fdc39",
      "360acb8bfee945c498bc947db22f10c6",
      "1bd7e46449624886af079319d21fa614",
      "11dc888d989e42a783094016f363228e",
      "3256d2fda3e14f6e8c8c51fa85f14bf5",
      "ea6eb06a9cbd4490a810073087fccac5",
      "f7c6bebf9c0b4d4aab1b7a6045bb2bd6",
      "3ba6e17241034b6cae0c04f69d5c1f27",
      "a2c5c7c6d56d4b61abbd2161a80bb216",
      "5e9de28fa2da453591c7021c180396ed",
      "5526206d621b46999cc694ce41a8bd75",
      "85e8c819f23e4f11a5b25a0a867f413f",
      "4fa08d13819b42318e790e9a897691aa",
      "bdf7369afcf14245b723a735828e414f",
      "1837a35abb7e433ca5db5d9a4f81617b",
      "f725339432bf4d31bd93b099811c21f9",
      "f6a44641bd6f469d985edc8b610eeb80",
      "04940f854f234546ac0371a432aabcbb",
      "fee50ecacd6240c4be8adfd433e96ee6",
      "b902d1e878d741149a0dab2b5852590b",
      "8a0bdd76e1684e8db0f29d60117de134",
      "62449b35158745fa8e2884f54f36cd21",
      "bc0b45d061564ffba5388186fd3ddf54",
      "1ce0622406094e9694d74661703982c7",
      "4c6fd73308ba4311a82612187d810fd8",
      "5577f2b3d7eb4d58bf3ff38f199e3957",
      "5df5f6cdd23349dead2cc46151c67651",
      "6710fa62f80f48e9b456e99d2776fc41",
      "6df03a79ca39470b9bb74b08b346a04f",
      "4923318a82534decb0a565d42191483a",
      "e4cb813a625e46589e192973b894bed2",
      "1b5409d86840457c8e35d5d2631655bb",
      "ba68062edf8d4c5d87bde1f559232b4a",
      "289b83d6b9ce48e1aa9ef7d3a8449eb1",
      "281fd11ab34749428f47bca0ff32a9eb",
      "f0378d01fde94b61bf014afcbb15a5a5",
      "138ca5cd7e854da8b6d92557e6fc5ce4",
      "d3ce82662f1d4dcb8db1ec59eb8a2803",
      "472b5819583b4febadbcdf7b49b6c4dc",
      "721620fe3d9a4546b4f5cdc038ef112a",
      "afeab63d4303409d826642f761484257",
      "e4ec234e1bca46ab970798987bfc8eee",
      "0491a5f857d24dc0ab9d3e22b600ef61",
      "eddffd88ded84b9ab23532474ef68453",
      "79249aade29f4ead9344232133f081e0",
      "5c014d8591ee4461ad26c3ba13d59d33",
      "8848cc2e33ce49199dbf9f1740944f7e",
      "cc8dccf2f9c64c1aaed993270565b3f8",
      "60a36b83765f4adcb14ed928b189436f",
      "442bce91f9314563bac898eb13f82c16",
      "9a0dfbb5c4d94b8d9f7ebe10ddb94133",
      "21d3a606c1484aeea4f5e1fb80f3f9b6",
      "b7cea3b0b57a4a488791880fd873bb6b",
      "4c542a26f38c4092933f3db0f5eb73b0",
      "852d2abfe4284c809b81d56148669015",
      "e209f375b10f46b2bb4b94e045364ac0",
      "5922156d65b5450687eac1842a48f7ef",
      "6693c60b4dd048238fa6bdda475a8ad8",
      "6f842c2b4c7f4763869c8b25e3068877",
      "2376c5378d924000b38e2caf7747f738",
      "78dcc7011ada44d0b63cf47be267f527",
      "89431daf376241a88bb49ba5a0043ef5",
      "f6b7326e9b4445858ec06dfc4b503000",
      "57184b2e1a6744f490cbde76c37197d9",
      "fe90094c5390479aa3fe548f253111eb",
      "349caa928264407eac76e43c9e22f4da",
      "ddce7b94b41d4c6cb8b126b42f0b262d",
      "bdd067f421dc432ab3b5af76292deb9e",
      "2d48f0ed49ee470087c333a8c561c652",
      "5dc542e8ff47421ab7cbda42ef337928",
      "5d48d7ee2590452ba4616b9c1a394818",
      "279ff03563274b94809ca88e9205effb",
      "68b0d58b751d4093ab72391f376841ae",
      "2026df480086491fa095b8b00d12f546"
     ]
    },
    "id": "eJZ_bbqblVLC",
    "outputId": "d4e9309b-aa14-42b6-aaab-ab698dc41b7e"
   },
   "outputs": [],
   "source": [
    "# --- Load Model and Processor ---\n",
    "print(f\"--- Loading model with Unsloth: {MODEL_ID} ---\")\n",
    "model, vision_processor = FastVisionModel.from_pretrained(\n",
    "    model_name=MODEL_ID,\n",
    "    load_in_4bit=True,                      # We can safely re-enable 4-bit!\n",
    "    use_gradient_checkpointing=\"unsloth\",   # True or \"unsloth\" for long context,\n",
    "    # max_seq_length=1024,                  # You can define this here\n",
    "    # dtype=torch.bfloat16,                 # Use bfloat16\n",
    "    # device_map=\"auto\",\n",
    ")\n",
    "print(\"‚úÖ Model and processor loaded and optimized by Unsloth.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0KO8QV2meTT"
   },
   "source": [
    "#### Configure LoRA (PEFT Adapters) with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBc546ZolVPF",
    "outputId": "ea2b82e6-37f6-41e5-eed3-ae70b20ec53e"
   },
   "outputs": [],
   "source": [
    "# --- Configure LoRA (PEFT Adapters) with Unsloth ---\n",
    "# Unsloth patches the model to prepare it for LoRA\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model=model,\n",
    "    finetune_vision_layers=True,      # False if not finetuning vision layers\n",
    "    finetune_language_layers=True,    # False if not finetuning language layers\n",
    "    finetune_attention_modules=True,  # False if not finetuning attention layers\n",
    "    finetune_mlp_modules=True,        # False if not finetuning MLP layers\n",
    "    r=16,                             # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha=16,                    # Recommended alpha == r at least\n",
    "    lora_dropout=0,                   # 0.05\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,                 # We support rank stabilized LoRA\n",
    "    loftq_config=None,                # And LoftQ\n",
    "    target_modules=\"all-linear\",      # Optional now! Can specify a list if needed\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")\n",
    "print(\"‚úÖ LoRA configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwdMRx3qmi6L"
   },
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be-l__PHlVTZ",
    "outputId": "49c30036-63e2-4831-8337-1284981f98e2"
   },
   "outputs": [],
   "source": [
    "# --- Load Dataset ---\n",
    "dataset = load_dataset(config)\n",
    "\n",
    "# print(\"\\n--- Decoding images in memory for the trainer ---\")\n",
    "# dataset = dataset.map(decode_images, batched=False)\n",
    "# print(\"‚úÖ Images decoded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TPIQVudWmZC"
   },
   "outputs": [],
   "source": [
    "# Inspect a sample from the dataset to understand the structure of the 'messages' field\n",
    "\n",
    "print(\"--- Inspecting a sample from the training dataset ---\")\n",
    "\n",
    "# Access a sample from the training dataset\n",
    "sample = dataset[\"train\"][0]\n",
    "print(\"\\nStructure of the first sample:\")\n",
    "print(sample)\n",
    "\n",
    "# Specifically look at the 'messages' field\n",
    "messages = sample.get(\"messages\")\n",
    "print(\"\\nStructure of the 'messages' field:\")\n",
    "print(messages)\n",
    "\n",
    "# Inspect the content within each message, focusing on image information\n",
    "print(\"\\nContent within each message:\")\n",
    "for i, message in enumerate(messages):\n",
    "    print(f\"\\nMessage {i}:\")\n",
    "    print(message)\n",
    "    content = message.get(\"content\")\n",
    "    if content:\n",
    "        for j, item in enumerate(content):\n",
    "            print(f\"  Item {j}:\")\n",
    "            print(item)\n",
    "            if item.get(\"type\") == \"image\":\n",
    "                print(\"    >>> Found image information here. Checking type:\")\n",
    "                print(f\"    >>> Type of 'image' field: {type(item.get('image'))}\")\n",
    "                print(f\"    >>> Value of 'image' field: {item.get('image')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOLZQdyYmyvx"
   },
   "source": [
    "#### Create custom logging callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NP91kYsLmve4"
   },
   "outputs": [],
   "source": [
    "# --- Create custom logging callback ---\n",
    "logging_callback = DetailedLoggingCallback(logging_dir=LOGGING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hZ4Tn6_m4jw"
   },
   "source": [
    "#### Configure Training using SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LzDlTuWpmvkZ",
    "outputId": "95c4e8dc-2d9d-4808-849c-7fa1f175f835"
   },
   "outputs": [],
   "source": [
    "# --- Configure Training using SFTConfig ---\n",
    "\n",
    "# Enable for training!\n",
    "FastVisionModel.for_training(model)\n",
    "\n",
    "print(\"\\n--- Configuring the SFTTrainer ---\")\n",
    "training_args = SFTConfig(\n",
    "    output_dir=TRAINER_OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,                            # Increased from 1 to 4!\n",
    "    gradient_accumulation_steps=4,                            # Can be reduced to 4 (Effective batch size = 16, 8)\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},   # Modern way to configure\n",
    "    max_grad_norm=0.3,                                        # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,\n",
    "    max_steps=30,\n",
    "    # num_train_epochs=1,                                     # Set this instead of max_steps for full training runs\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=2e-4,                                       # 2e-4, 2e-5\n",
    "    save_strategy=\"steps\",                                    # epoch, steps\n",
    "    eval_strategy=\"steps\",                                    # no, epoch, steps\n",
    "    logging_steps=10,                                         # 1, 10, 50\n",
    "    logging_dir=LOGGING_DIR,                                  # Specify logging directory\n",
    "    logging_first_step=True,                                  # Log the first step\n",
    "    eval_steps=10,                                            # Evaluate every 50 steps (in addition to epoch)\n",
    "    save_steps=30,                                            # Save checkpoint every 100 steps\n",
    "    load_best_model_at_end=True,                              # Load best model at end of training\n",
    "    metric_for_best_model=\"eval_loss\",                        # Use eval loss to determine best model\n",
    "    greater_is_better=False,                                  # Lower eval loss is better\n",
    "    optim=\"adamw_torch_fused\",                                # Use the standard optimizer, adamw_8bit, adamw_torch, adamw_torch_fused\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=3407,\n",
    "    # bf16=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"tensorboard\"],                                # Use list format for multiple reporters\n",
    "    # packing=False,\n",
    "    # Additional logging configurations\n",
    "    dataloader_pin_memory=False,                              # Can help with performance\n",
    "    dataset_text_field=\"\",\n",
    "    remove_unused_columns=False,                              # Keep all columns for debugging\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    max_length=2048,  # 1024\n",
    ")\n",
    "\n",
    "# --- Chat Template ---\n",
    "processor = get_chat_template(vision_processor, \"gemma-3\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"val\"],\n",
    "    processing_class=processor.tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, processor),\n",
    "    callbacks=[logging_callback],                             # Add custom logging callback\n",
    "    args=training_args,\n",
    "    # formatting_func=formatting_func,\n",
    ")\n",
    "print(\"‚úÖ SFTTrainer configured with enhanced logging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7xg1leRm_B0"
   },
   "source": [
    "#### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ky4DEoEcmvo5",
    "outputId": "0144bc30-95e2-4064-c871-879f4b47794a"
   },
   "outputs": [],
   "source": [
    "# --- Start Training ---\n",
    "start_gpu_memory: float = 0.0\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()                                                # Reset stats to get a clean measurement for the training phase\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(\"\\nüöÄ --- Starting the fine-tuning process --- üöÄ\")\n",
    "print(\"-\" * 47)\n",
    "print(f\"TensorBoard logs will be saved to: {LOGGING_DIR}\")\n",
    "print(f\"Training checkpoints will be saved to: {TRAINER_OUTPUT_DIR}\")\n",
    "\n",
    "# Configure and setup TensorBoard based on config settings\n",
    "setupTensorboard(config, LOGGING_DIR)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "os.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\"\n",
    "training_start_time = time.time()\n",
    "trainer_stats = trainer.train()\n",
    "training_end_time = time.time()\n",
    "training_duration = training_end_time - training_start_time\n",
    "\n",
    "print(f\"\\nüèÅ --- Fine-tuning complete --- üèÅ\")\n",
    "print(\n",
    "    f\"Total training time: {training_duration/3600:.2f} hours ({training_duration/60:.1f} minutes)\"\n",
    ")\n",
    "\n",
    "# --- Calculate and Display GPU Memory Usage (After training) ---\n",
    "memory_stats = get_gpu_usage_stats(start_gpu_memory=start_gpu_memory)\n",
    "\n",
    "print(\"\\n--- GPU Memory Usage Summary ---\")\n",
    "# The function handles the check, but we can check again for a cleaner print message\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Peak reserved memory = {memory_stats['peak_memory_gb']} GB.\")\n",
    "    print(\n",
    "        f\"Peak reserved memory for training = {memory_stats['training_memory_gb']} GB.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Peak reserved memory % of max memory = {memory_stats['peak_memory_percent']} %.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Peak reserved memory for training % of max memory = {memory_stats['training_memory_percent']} %.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No GPU was used, so no memory stats were recorded.\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxnTQn-SnPyn"
   },
   "source": [
    "#### Analyze and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZLL_2UAmvtS",
    "outputId": "ab9f6303-8c81-4a8b-cf7d-286cd4afecf4"
   },
   "outputs": [],
   "source": [
    "# --- Analyze and Display Results ---\n",
    "print(\"\\n--- Training Performance Summary ---\")\n",
    "training_summary = display_training_summary(trainer_stats)\n",
    "print(training_summary)\n",
    "\n",
    "print(\"\\n--- Evaluation Performance Summary ---\")\n",
    "evaluation_summary = display_evaluation_summary(trainer)\n",
    "print(evaluation_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hwqq_L0XncK-"
   },
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UkBojxknfZF"
   },
   "source": [
    "##### Save the Fine-Tuning Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_JJuNadlVXK",
    "outputId": "6ad07a10-dad6-428b-ae50-efab796d4652"
   },
   "outputs": [],
   "source": [
    "# --- Save the Fine-Tuning Artifacts ---\n",
    "print(\n",
    "    f\"\\n--- Saving lightweight LoRA adapters and processor to: {FINAL_ADAPTERS_OUTPUT_DIR} ---\"\n",
    ")\n",
    "# This saves the tiny adapter files (your \"blueprint\" for the changes)\n",
    "trainer.save_model(FINAL_ADAPTERS_OUTPUT_DIR)\n",
    "# This saves the processor files (tokenizer.json, preprocessor_config.json, etc.)\n",
    "# into the exact same directory, making it a complete, self-contained model folder.\n",
    "vision_processor.save_pretrained(FINAL_ADAPTERS_OUTPUT_DIR)\n",
    "print(f\"‚úÖ Fine-tuned artifacts saved successfully to {FINAL_ADAPTERS_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKHJM8h2nlD3"
   },
   "source": [
    "##### Export To HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651,
     "referenced_widgets": [
      "1d0ecf7fafb14de3a7683bc6adbfa8d7",
      "67475cfa5dd84187996cb937befbbe6a",
      "d3fc31dff2f54862931a63f85574eae8",
      "e22f58c5039c4793b2633baed65eb364",
      "d052a19ed9384060adabab1a430ac0f4",
      "c7f4acb674c54b9faa82748560a7e41b",
      "9352eea8bf6f452498312790b6ab4acd",
      "0ac72bccba3a4db9a6b712d7ea25ad06",
      "7284c243b416407f8af8ee01e3d31437",
      "0ecc3662536d4d0b97876871ae7cbbca",
      "e20f2006c10a4f17b9b32be9669dfe4e",
      "6e7606f566cb41489627697dbbb4d947",
      "063207feb974471f99c48627b115b49b",
      "044953bebbf243dc9bae62a59559c86a",
      "cf9b67a957604c8dbad536a3fa9dd3e4",
      "b5d1097ae1c54ff0baeb8ea5b7e88ce0",
      "7acb34aa1fec41a48316af0960aaa808",
      "abdb108f60714bc78e3330e70651d78f",
      "42c5b5e2b3994fbeb3a25122baaba2eb",
      "f2e4f4ec5d234f8c92548d1ff71ab6bf",
      "7437a6d733c0422b8e2adf84da4a8d66",
      "efb6ba7874f04c288e3a36830620e74d",
      "11435d162522495199102db916673d83",
      "b03cb98aa5f549cb9d15f73bf998a738",
      "5c892f00038042e4962f3ab1ea61e4fc",
      "f8f9dac1d6fd40c5bfcf0b5f074f6993",
      "a82faf21bc474e9388305d1b3743c005",
      "4b9ea2bd391a4b98bca6e87099eaa96c",
      "b7abbb33d2594a539842a61d0033adb5",
      "19333805ab454f2a95bb68480911a58e",
      "4f2238c9bfef4059a1bea5a08e98e022",
      "cc84ba7e807b466da7aca34e54d0f502",
      "46be3bd0295847218d83c295fad4389d",
      "c47f2c953a8a472abeff9edc15aed01b",
      "30ed21e67a2e41929aa54131260ff800",
      "be7e0c5f1f7843dda91b382fc626bbf1",
      "be6a60e243d44902a2c2a4e920f85ef8",
      "3c179f8d09e846a7870c70bee15b0df6",
      "4263eb353a23424fbc9c1d1459d58513",
      "66dd4ecdf3ba4652ac9b467a19172f5b",
      "d8022c1074c9435781aa64c3f0f6b4c7",
      "bf2c6a1422be4ff0ae9cafa363c84bab",
      "af592854fa594857acd2d4f7c7035d03",
      "275a71d5f3244109b59b2af0b0cb57b8",
      "7aa8c7bd57b74e9f98fabf2a058fdf24",
      "c0f464a1b7e843e0a4f6db385b0fd907",
      "8ea17e64b3724722996c4f21e52d60b3",
      "3a2f9bdf037c4aeaad50a96b32b4e613",
      "8e0dbaf5a5394543a8055534390bb268",
      "a0d6f2a226174568a4c9b073e1b745f1",
      "f3663c8edcec483a9cc810c30fdf931f",
      "4cea6675781542fabf6886e8a2c0d2d9",
      "13459b706fc845079fe62468c081d72e",
      "2c1420581f2d4bb89af723078efd2984",
      "e120a17809df4f5b903d1a674abdfc74",
      "75e2225bd6ea4b63b26d5ec9a7441561",
      "02465c24c2c5491893442125580d5373",
      "48f4e34f411b4e72a5d69559a7812cca",
      "ceda36ca1560456ca6c7b71cfbc4ec01",
      "8604afcda8a64ec689c95de7e2053c50",
      "96b0e3105b8145f6bcd2317e192a57d9",
      "c7885eaf817c48eeb20da2dfdf71c90d",
      "2e4e53a6002f4f3a9995ab1a7b671001",
      "b7c21ecd5f3b4f32baf99f1d88f3f99f",
      "4306f0a2aba94edfa2a1b1d0d16569c7",
      "0e3e62c5221444a6ab96bd706894ac5a",
      "f946e416807747b7b04bfd45c6453071",
      "f166cabe04b446808818d822c155370c",
      "20ecc96840b74c4fa4e72a5f3b1f9e7f",
      "0b4e5ba57df94df59380c540057e60ea",
      "771475ebd5b744ec8ef119cad5573fe7",
      "e69b9713721540d38107c94d40b875f2",
      "40cfe8626859406dbbeb0ac8ddbe559d",
      "009870cc2f5945369b947d52c3131605",
      "abb8b80413e74afda7c19f7082a9aba1",
      "846a210d718945b2af2522a78d1071d6",
      "573b36b10a4f4e27bc6f9ed6b0aa15c2",
      "732a472ed88d4f23b4f56f9e31e510ee",
      "3f6e29fa8c854206878dace8b864c835",
      "f2224189cd9d472cb9cb162a789ca76d",
      "9ba9f385ce184e69929f14b6bfe92b0b",
      "7dc6b1ac18974c91931b1b3023a517ac",
      "a8661834ade44d3a809c7db63a755638",
      "12f91b7d105449a89bd28a95af389187",
      "1f3394bd0cf244c3aab5ba41a106a804",
      "05d22938ea7f41978af7e60481838392",
      "c7b99924cf2243b7a7c1b9b5f1855b2a",
      "ac766610f2814ba8b0e3201ea5901155",
      "dc67f27fc97d4d02858b8d32d3346cf6",
      "2239062dd0ec46acbe880c82091d5628",
      "b5023633d3544721b85f9da83f5cec13",
      "6f16e34ec61f46c39021d684f82ed19e",
      "e8265ef3e36a4ce88e4f8d4d19e6b63c",
      "fc00a2f9cf3744f99c58df03321d633a",
      "48456f6739d84b49aacd3d5dcc3879a5",
      "1e9e8b1c8076493498d1b30c4decd90f",
      "5d252831ac1c4196b1b28654322f7e2a",
      "184f9ae25ee846efaec465278e3c01a1",
      "931ca9d5bfd64354bfda299f07e748f9",
      "df3e39a22c7d4d078b84b089ee2216d1",
      "c5af758b0695472aa940aef54f05c001",
      "0757986bd3274a27ad46dfcdac86b641",
      "46d1e6104ab644e0b7abe47a4745077f",
      "4aa3d6bc4c444a3ea1f9723b192e6bf0",
      "b525f4a286ce4c7abd8a520e36691d6c",
      "ff56eb81c8c64056a0b7ffbac202b6f2",
      "30178ec3f3e24cdb8de7faed9b589c9b",
      "0d28c09936d440339e60548a95438d01",
      "cdb632b026234c96a0002f080f625f01",
      "9e5659a01c174495b466fb6c18da5481"
     ]
    },
    "id": "KZHnYM6zno8Y",
    "outputId": "443764e8-1749-4885-f1bd-7d97abb80094"
   },
   "outputs": [],
   "source": [
    "# --- Export Merged Model to Hugging Face Hub ---\n",
    "# Get the export configuration from your YAML file\n",
    "export_config = config.get(\"fineTune\", {}).get(\"export\", {})\n",
    "# Conditionally push the full, merged model to the Hub\n",
    "if export_config.get(\"push_to_hub\", False):\n",
    "    push_merged_model_to_hub(model, vision_processor, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1m3TmenDBaY"
   },
   "outputs": [],
   "source": [
    "# # Start the fine-tuning process...\n",
    "# print(\"üöÄ Running Model training process...\")\n",
    "# %run /content/fineTune/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjItXK-2Btfa"
   },
   "outputs": [],
   "source": [
    "# help(SFTTrainer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
